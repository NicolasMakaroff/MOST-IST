{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "import math\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import data_gestion\n",
    "import stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_gestion.open_data('preprocessed_train_data.csv')                                         \n",
    "tmp = data.pop('Unnamed: 0')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['trip_duration'] = np.log(data['trip_duration'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['blizzard'] = data['blizzard'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(data.isna(),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace([np.inf, -np.inf], np.nan).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, test_data, test_labels = data_gestion.create_train_test_set(data,0.2,0.1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn_params = {\n",
    "    'n_neighbors': 4\n",
    "}\n",
    "taxi_knn = KNeighborsRegressor(n_neighbors = 20)\n",
    "taxi_knn.fit(train_data,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_predictions = taxi_knn.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "rmsle = np.sqrt(mean_squared_error(knn_predictions, test_labels))\n",
    "print(rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsle_val = [] #to store rmse values for different k\n",
    "for K in range(20,40):\n",
    "    K = K+1\n",
    "    model = KNeighborsRegressor(n_neighbors = K)\n",
    "\n",
    "    model.fit(train_data, train_labels)  #fit the model\n",
    "    pred=model.predict(test_data) #make prediction on test set\n",
    "    error = np.sqrt(mean_squared_error(test_labels,pred)) #calculate rmse\n",
    "    rmsle_val.append(error) #store rmse values\n",
    "    print('RMLSE value for k= ' , K , 'is:', error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Function to mesure the quality of a split\n",
    "criterion = ['friedman_mse','mse']\n",
    "\n",
    "# Strategy to choose to split\n",
    "splitter = ['best','random']\n",
    "\n",
    "# Maximum depth\n",
    "max_depth = [int(x) for x in np.linspace(10, 300, num = 60)]\n",
    "max_depth.append(None)\n",
    "\n",
    "min_sample_split = [int(x) for x in np.linspace(1,50,num=50)]\n",
    "\n",
    "min_sample_leaf = [int(x) for x in np.linspace(1,50,num=50)]\n",
    "\n",
    "min_weight_fraction_leaf = [0.0,0.1,0.2]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "\n",
    "max_leaf_nodes = [10,20,30,40,50,None]\n",
    "\n",
    "min_impurity_decrease = [0.0,0.1,0.2]\n",
    "\n",
    "ccp_alpha = [0.0,0.1,0.2]\n",
    "\n",
    "random_grid = {'criterion': criterion,\n",
    "               'splitter': splitter,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_sample_split,\n",
    "               'min_samples_leaf': min_sample_leaf,\n",
    "               'min_weight_fraction_leaf': min_weight_fraction_leaf,\n",
    "               'max_features' : max_features,\n",
    "               'max_leaf_nodes': max_leaf_nodes,\n",
    "               'min_impurity_decrease': min_impurity_decrease,\n",
    "               'ccp_alpha': ccp_alpha}\n",
    "\n",
    "print(random_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "tree_random = RandomizedSearchCV(estimator = tree, param_distributions = random_grid, n_iter = 300, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "tree_random.fit(train_data, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_random.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = DecisionTreeRegressor(random_state = 42)\n",
    "base_model.fit(train_data, train_labels)\n",
    "pes_predictions_base = base_model.predict(test_data)\n",
    "base_accuracy = np.sqrt(mean_squared_error(test_labels,pes_predictions_base))\n",
    "\n",
    "print(\"Root mean squared error: %.4f \"\n",
    "      % np.sqrt(mean_squared_error(test_labels,pes_predictions_base)))\n",
    "\n",
    "\n",
    "best_random = tree_random.best_estimator_\n",
    "pes_predictions_best = best_random.predict(test_data)\n",
    "random_accuracy = np.sqrt(mean_squared_error(test_labels,pes_predictions_best))\n",
    "\n",
    "print(\"Root mean squared error: %.4f MeV\"\n",
    "      % np.sqrt(mean_squared_error(test_labels,pes_predictions_best)))\n",
    "\n",
    "print('Improvement of {:0.4f}%.'.format( 100 * (base_accuracy - random_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "tree_reg = tree_random.best_estimator_\n",
    "predictions = tree_reg.predict(test_data)\n",
    "print(np.sqrt(mean_squared_error(test_labels,predictions)))\n",
    "\n",
    "importances = tree_reg.feature_importances_\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Rearrange feature names so they match the sorted feature importances\n",
    "names = [list(train_data)[i] for i in indices]\n",
    "\n",
    "# Create plot\n",
    "plt.figure()\n",
    "\n",
    "# Create plot title\n",
    "plt.title(\"Feature Importance\")\n",
    "\n",
    "# Add bars\n",
    "plt.bar(range(train_data.shape[1]), importances[indices])\n",
    "\n",
    "# Add feature names as x-axis labels\n",
    "plt.xticks(range(train_data.shape[1]), names, rotation=90)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = create_train_test_set(data,1.,0.)  \n",
    "\n",
    "tree_reg = RandomForestRegressor(n_jobs = -1, \n",
    "                                n_estimators = 56,\n",
    "                                min_samples_split = 2,\n",
    "                                min_samples_leaf = 1,\n",
    "                                max_features = 'auto',\n",
    "                                max_depth = 50,\n",
    "                                bootstrap = True,\n",
    "                                verbose = 0)\n",
    "\n",
    "tree_reg.fit(train_data,train_labels)\n",
    "\n",
    "test = pd.read_csv('preprocessed_test_data.csv')\n",
    "#test = test.replace([np.inf, -np.inf], np.nan)\n",
    "test.pop('Unnamed: 0')                                                                                                   \n",
    "\n",
    "np.sum(test.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree_reg = tree_random.best_estimator_\n",
    "test_predictions = tree_reg.predict(test)\n",
    "test_predictions = np.exp(test_predictions) - 1\n",
    "test_sample = pd.read_csv('nyc-taxi-trip-duration/test.csv')\n",
    "df = pd.DataFrame(test_predictions, columns = ['trip_duration'])\n",
    "my_submission = pd.DataFrame({'id' : test_sample['id'], 'trip_duration' : df['trip_duration']})\n",
    "my_submission.to_csv('first_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg = DecisionTreeRegressor().fit(train_data,train_labels)\n",
    "predictions = tree_reg.predict(test_data)\n",
    "print(np.sqrt(mean_squared_error(test_labels,predictions)))\n",
    "importances = tree_reg.feature_importances_\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Rearrange feature names so they match the sorted feature importances\n",
    "names = [list(train_data)[i] for i in indices]\n",
    "\n",
    "# Create plot\n",
    "plt.figure()\n",
    "\n",
    "# Create plot title\n",
    "plt.title(\"Feature Importance\")\n",
    "\n",
    "# Add bars\n",
    "plt.bar(range(train_data.shape[1]), importances[indices])\n",
    "\n",
    "# Add feature names as x-axis labels\n",
    "plt.xticks(range(train_data.shape[1]), names, rotation=90)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {\n",
    "    'n_neighbors': 30\n",
    "}\n",
    "taxi_knn = KNeighborsRegressor(n_neighbors = 30)\n",
    "taxi_knn.fit(train_data,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "knn_predictions = taxi_knn.predict(test_data)\n",
    "rmsle = np.sqrt(mean_squared_error(knn_predictions, test_labels))\n",
    "print(rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "C = [int(x) for x in np.linspace(start = 1, stop = 100, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "epsilon = [.2,.3,.4,.5,.6]\n",
    "\n",
    "random_grid = {'C': C,\n",
    "               'epsilon': epsilon}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "svr_random = RandomizedSearchCV(estimator = svr, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "svr_random.fit(train_data, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = SVR(C=900)\n",
    "base_model.fit(train_data, train_labels)\n",
    "pes_predictions_base = base_model.predict(test_data)\n",
    "base_accuracy = np.sqrt(mean_squared_error(test_labels,pes_predictions_base))\n",
    "\n",
    "print(\"Root mean squared error: %.2f \"\n",
    "      % np.sqrt(mean_squared_error(test_labels,pes_predictions_base)))\n",
    "\n",
    "\n",
    "best_random = svr_random.best_estimator_\n",
    "pes_predictions_best = best_random.predict(test_data)\n",
    "random_accuracy = np.sqrt(mean_squared_error(test_labels,pes_predictions_best))\n",
    "\n",
    "print(\"Root mean squared error: %.2f MeV\"\n",
    "      % np.sqrt(mean_squared_error(test_labels,pes_predictions_best)))\n",
    "\n",
    "print('Improvement of {:0.2f}%.'.format( 100 * (base_accuracy - random_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "                                                              \n",
    "train_data, train_labels, test_data, test_labels = data_gestion.create_train_test_set(data,0.8,0.2)             \n",
    "\n",
    "model = stacking.StackedRegressor([DecisionTreeRegressor(criterion='friedman_mse', \n",
    "                                                max_depth=41,\n",
    "                                                max_features='auto',\n",
    "                                                max_leaf_nodes=None,\n",
    "                                                min_samples_leaf=20, \n",
    "                                                min_samples_split=10,\n",
    "                                                splitter='random'),\n",
    "                         DecisionTreeRegressor(criterion='mse', \n",
    "                                                max_depth=30,\n",
    "                                                max_features='auto',\n",
    "                                                max_leaf_nodes=None,\n",
    "                                                min_samples_leaf=20, \n",
    "                                                min_samples_split=10,\n",
    "                                                splitter='random'),\n",
    "                         DecisionTreeRegressor(criterion='friedman_mse', \n",
    "                                                max_depth=60,\n",
    "                                                max_features='auto',\n",
    "                                                max_leaf_nodes=None,\n",
    "                                                min_samples_leaf=20, \n",
    "                                                min_samples_split=10,\n",
    "                                                splitter='random'),\n",
    "                         RandomForestRegressor(n_jobs = -1, \n",
    "                                n_estimators = 56,\n",
    "                                min_samples_split = 2,\n",
    "                                min_samples_leaf = 1,\n",
    "                                max_features = 'auto',\n",
    "                                max_depth = 50,\n",
    "                                bootstrap = True,\n",
    "                                verbose = 0)])\n",
    "\n",
    "# Performance of ensemble\n",
    "stacking.cross_val_metric(model, train_data, train_labels,metric=stacking.root_mean_squared_error,cv=3, display='RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 2, stop = 100, num = 50)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)\n",
    "\n",
    "RandomForestRegressor(n_jobs = -1, \n",
    "                                n_estimators = 56,\n",
    "                                min_samples_split = 2,\n",
    "                                min_samples_leaf = 1,\n",
    "                                max_features = 'auto',\n",
    "                                max_depth = 50,\n",
    "                                bootstrap = True,\n",
    "                                verbose = 0),\n",
    "                          RandomForestRegressor(n_jobs = -1, \n",
    "                                n_estimators = 100,\n",
    "                                min_samples_split = 5,\n",
    "                                min_samples_leaf = 1,\n",
    "                                max_features = 'auto',\n",
    "                                max_depth = 30,\n",
    "                                bootstrap = True,\n",
    "                                verbose = 0),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=2, random_state=42, n_jobs = -1)# Fit the random search model\n",
    "rf_random.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = RandomForestRegressor(n_jobs = -1, \n",
    "                                n_estimators = 56,\n",
    "                                min_samples_split = 2,\n",
    "                                min_samples_leaf = 1,\n",
    "                                max_features = 'auto',\n",
    "                                max_depth = 50,\n",
    "                                bootstrap = True,\n",
    "                                verbose = 0)\n",
    "base_model.fit(train_data, train_labels)\n",
    "pes_predictions_base = base_model.predict(test_data)\n",
    "base_accuracy = np.sqrt(mean_squared_error(test_labels,pes_predictions_base))\n",
    "\n",
    "print(\"Root mean squared error: %.5f \"\n",
    "      % np.sqrt(mean_squared_error(test_labels,pes_predictions_base)))\n",
    "\n",
    "\n",
    "best_random = rf_random.best_estimator_\n",
    "pes_predictions_best = best_random.predict(test_data)\n",
    "random_accuracy = np.sqrt(mean_squared_error(test_labels,pes_predictions_best))\n",
    "\n",
    "print(\"Root mean squared error: %.5f MeV\"\n",
    "      % np.sqrt(mean_squared_error(test_labels,pes_predictions_best)))\n",
    "\n",
    "print('Improvement of {:0.5f}%.'.format( 100 * (base_accuracy - random_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('preprocessed_test_data.csv')\n",
    "#test = test.replace([np.inf, -np.inf], np.nan)\n",
    "test.pop('Unnamed: 0')                                                                                                   \n",
    "\n",
    "np.sum(test.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = test.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "test_predictions = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(test)\n",
    "test_predictions = np.exp(test_predictions) - 1\n",
    "test_sample = pd.read_csv('nyc-taxi-trip-duration/test.csv')\n",
    "df = pd.DataFrame(test_predictions, columns = ['trip_duration'])\n",
    "my_submission = pd.DataFrame({'id' : test_sample['id'], 'trip_duration' : df['trip_duration']})\n",
    "my_submission.to_csv('first_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = pd.read_csv('nyc-taxi-trip-duration/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(test_predictions, columns = ['trip_duration'])\n",
    "my_submission = pd.DataFrame({'id' : test_sample['id'], 'trip_duration' : df['trip_duration']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission.to_csv('first_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('nyc-taxi-trip-duration/sample_submission.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import xgboost as xgb\n",
    "\n",
    "ntrain = train_data.shape[0]\n",
    "ntest = test_data.shape[0]\n",
    "SEED = 0 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold prediction\n",
    "kf = KFold( n_splits = NFOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put in our parameters for said classifiers\n",
    "# Random Forest parameters\n",
    "\n",
    "rf_params = {\n",
    "    'n_jobs': -1, \n",
    "    'n_estimators': 56,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'max_features': 'auto',\n",
    "    'max_depth': 50,\n",
    "    'bootstrap': True,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Extra Trees Parameters\n",
    "et_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators':500,\n",
    "    #'max_features': 0.5,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate' : 0.7\n",
    "}\n",
    "\n",
    "  \n",
    "# Gradient Boosting parameters\n",
    "gb_params = {\n",
    "    \n",
    "}\n",
    "rf_1_params = {\n",
    "    'bootstrap' : True,\n",
    "    'max_depth' : 40,\n",
    "    'max_features' : 'auto',\n",
    "    'min_samples_leaf' : 1,\n",
    "    'min_samples_split' : 2,\n",
    "    'n_estimators' : 102,\n",
    "    'n_jobs' : -1\n",
    "    \n",
    "} \n",
    "\n",
    "rf_2_params = {\n",
    "    'bootstrap' : True,\n",
    "    'max_depth' : 50,\n",
    "    'max_features' : 'auto',\n",
    "    'min_samples_leaf' : 1,\n",
    "    'min_samples_split' : 2,\n",
    "    'n_estimators' : 56,\n",
    "    'n_jobs' : -1\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "    \n",
    "tree_1_params = {\n",
    "    'criterion':'friedman_mse', \n",
    "    'max_depth':41,\n",
    "    'max_features':'auto',\n",
    "    'max_leaf_nodes':None,\n",
    "    'min_samples_leaf':20, \n",
    "    'min_samples_split':10,\n",
    "    'splitter':'random'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Support Vector Classifier parameters \n",
    "svc_params = {\n",
    "    'epsilon': 0.7,\n",
    "    'C': 1000\n",
    "    }\n",
    "\n",
    "knn_params = {\n",
    "    'n_neighbors': 4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# Create 5 objects that represent our 4 models\n",
    "rf = stacking.SklearnHelper(clf=RandomForestRegressor, seed=SEED, params=rf_params)\n",
    "rf1 = stacking.SklearnHelper(clf=RandomForestRegressor, seed=SEED, params=rf_1_params)\n",
    "rf2 = stacking.SklearnHelper(clf=RandomForestRegressor, seed=SEED, params=rf_2_params)\n",
    "tree = stacking.SklearnHelper(clf=DecisionTreeRegressor, seed=SEED,params=tree_1_params)\n",
    "#et = SklearnHelper(clf=ExtraTreesRegressor, seed=SEED, params=et_params)\n",
    "#ada = SklearnHelper(clf=AdaBoostRegressor, seed=SEED, params=ada_params)\n",
    "#gb = SklearnHelper(clf=GradientBoostingRegressor, seed=SEED, params=gb_params)\n",
    "#svc = SklearnHelper(clf=SVR, seed=SEED, params=svc_params)\n",
    "#knn = SklearnHelper(clf=KNeighborsRegressor, seed=SEED, params=knn_params)\n",
    "time5 = time.time() - start_time\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "train_data, train_labels, test_data, test_labels = create_train_test_set(data,0.9,0.1)             \n",
    "normed_train_features = train_data.to_numpy()\n",
    "normed_test_features = test_data.to_numpy()\n",
    "train_labels = train_labels.to_numpy()\n",
    "test_labels = test_labels.to_numpy()\n",
    "time6 = time.time() - start_time\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Create our OOF train and test predictions. These base results will be used as new features\n",
    "#et_oof_train, et_oof_test = get_oof(et, normed_train_features, train_labels, normed_test_features) # Extra Trees\n",
    "#print('OK')\n",
    "rf_oof_train, rf_oof_test = stacking.get_oof(rf,normed_train_features, train_labels, normed_test_features) # Random Forest\n",
    "print('OK')\n",
    "rf1_oof_train, rf1_oof_test = stacking.get_oof(rf1,normed_train_features, train_labels, normed_test_features) # Random Forest\n",
    "print('OK')\n",
    "rf2_oof_train, rf2_oof_test = stacking.get_oof(rf2,normed_train_features, train_labels, normed_test_features) # Random Forest\n",
    "print('OK')\n",
    "tree_oof_train, tree_oof_test = stacking.get_oof(tree,normed_train_features, train_labels, normed_test_features) # Random Forest\n",
    "print('OK')\n",
    "#ada_oof_train, ada_oof_test = stacking.get_oof(ada, normed_train_features, train_labels, normed_test_features) # AdaBoost \n",
    "#print('OK')\n",
    "#gb_oof_train, gb_oof_test = stacking.get_oof(gb,normed_train_features, train_labels, normed_test_features) # Gradient Boost\n",
    "#print('OK')\n",
    "#svc_oof_train, svc_oof_test = stacking.get_oof(svc,normed_train_features, train_labels, normed_test_features) # Support Vector Classifier\n",
    "#print('OK')\n",
    "#knn_oof_train, knn_oof_test = stacking.get_oof(knn,normed_train_features,train_labels,normed_test_features)  # kNN\n",
    "print(\"Training is complete\")\n",
    "time7 = time.time() - start_time\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "\n",
    "data = [\n",
    "    go.Heatmap(\n",
    "        z= base_predictions_train.astype(float).corr().values ,\n",
    "        x=base_predictions_train.columns.values,\n",
    "        y= base_predictions_train.columns.values,\n",
    "          colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            reversescale = True\n",
    "    )\n",
    "]\n",
    "py.iplot(data, filename='labelled-heatmap')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "x_train = np.concatenate((rf_oof_train, rf1_oof_train,rf2_oof_train,tree_oof_train), axis=1)\n",
    "x_test = np.concatenate((rf_oof_test,rf1_oof_test,rf2_oof_test,tree_oof_test), axis=1)\n",
    "time8 = time.time() - start_time\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('preprocessed_test_data.csv')\n",
    "#test = test.replace([np.inf, -np.inf], np.nan)\n",
    "test.pop('Unnamed: 0')                                                                                                   \n",
    "\n",
    "\"\"\"start_time = time.time()\n",
    "gbm = xgb.XGBRegressor(\n",
    "     learning_rate = 0.01,\n",
    "     n_estimators= 1000,\n",
    "     max_depth= 110,\n",
    "     min_child_weight= 5,\n",
    "     gamma=.5,                        \n",
    "     subsample=1.,\n",
    "     colsample_bytree=1.,\n",
    "     nthread= -1,\n",
    "     scale_pos_weight=1).fit(x_train, train_labels)\n",
    "\n",
    "predictions = gbm.predict(x_test)\n",
    "time9 = time.time() - start_time\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\"\"\"\n",
    "br = BayesianRidge().fit(x_train,train_labels)\n",
    "test_predictions = br.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_predictions = model.predict(test)\n",
    "test_predictions = np.exp(test_predictions) - 1\n",
    "test_sample = pd.read_csv('nyc-taxi-trip-duration/test.csv')\n",
    "df = pd.DataFrame(test_predictions, columns = ['trip_duration'])\n",
    "my_submission = pd.DataFrame({'id' : test_sample['id'], 'trip_duration' : df['trip_duration']})\n",
    "my_submission.to_csv('first_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsle = np.sqrt(mean_squared_error(predictions,test_labels))\n",
    "print(rmsle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('preprocessed_test_data.csv')\n",
    "#test = test.replace([np.inf, -np.inf], np.nan)\n",
    "test.pop('Unnamed: 0')                                                                                                   \n",
    "\n",
    "np.sum(test.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = create_train_test_set(data,0.9,0.1)  \n",
    "test_predictions = stacking.stacking(train_data, train_labels, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_predictions = model.predict(test)\n",
    "test_predictions = np.exp(test_predictions) - 1\n",
    "test_sample = pd.read_csv('nyc-taxi-trip-duration/test.csv')\n",
    "df = pd.DataFrame(test_predictions, columns = ['trip_duration'])\n",
    "my_submission = pd.DataFrame({'id' : test_sample['id'], 'trip_duration' : df['trip_duration']})\n",
    "my_submission.to_csv('first_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbaseconda7bdb38472b8447b1a3c551411def93ed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
